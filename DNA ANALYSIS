//  hadoop jar Prithvi/DNA_Prithvi.jar 
//      Driver_DNA 
//      /user/student/Prithvi/DNA/Input 
//      /user/student/Prithvi/DNA/Output

//DRIVER CLASS
import org.apache.hadoop.fs.Path;
//import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class Driver_DNA 
	{

		public static void main(String[] args) 
				throws Exception 	
					{
						if (args.length != 2) {
						System.out.printf("Usage: StubDriver <input dir> <output dir>\n");
						System.exit(-1);
					}
				
				JobConf conf = new JobConf();
				@SuppressWarnings("deprecation")
				Job job = new Job(conf, "wordcount");
				job.setJarByClass(Driver_DNA.class);
				job.setMapperClass(Mapper_DNA.class);
				job.setReducerClass(Reducer_DNA.class);
			
			//Map reduce Output
				
				job.setOutputKeyClass(Text.class);
				job.setOutputValueClass(Text.class);
				
			//Map outputs
				job.setMapOutputKeyClass(Text.class);
				job.setMapOutputValueClass(Text.class);
		
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		boolean result = job.waitForCompletion(true);
		System.exit(result ? 0 : 1);
	}
	}

// MAPPER CLASS
import java.io.IOException;
//import java.util.Arrays;
//import org.apache.hadoop.io.IntWritable;
//import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class Mapper_DNA  extends Mapper<Object, Text, Text, Text> {

  @Override
  public void map(Object key, Text value, Context context)
      throws IOException, InterruptedException {

	  String[] words = value.toString().split("[\n]");
	  for(String word:words)
	  {
		  word = word.replace("\"", "") ;
		context.write(new Text(word.substring(6)), new Text(word.substring(0,5)));	  
	  }
  }
}

// REDUCER CLASS
import java.io.IOException;
//import org.apache.hadoop.io.DoubleWritable;
//import org.apache.hadoop.io.IntWritable;
//import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class  Reducer_DNA extends Reducer<Text, Text, Text, Text> {

@Override
public void reduce(Text key, Iterable<Text> values, Context context)
    throws IOException, InterruptedException 
	{
		String op = new String("");
		for(Text iw:values)
			{
				String value1 = iw.toString();
				op = op + ", " + value1;
			}
	 context.write(key, new Text(op.substring(2)));
}
}
